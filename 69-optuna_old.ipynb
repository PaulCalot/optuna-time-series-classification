{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OPTUNA \n",
    "\n",
    "MIT licence.\n",
    "\n",
    "3 principaux objectifs :\n",
    "1. Construction de l'espace de recherche de façon dynamique (*define-by-run API*)\n",
    "2. Implémentation efficiente de la recherche mais également du *pruning* (c'est-à-dire quand des branches entières de l'espace de recherche sont supprimées)\n",
    "3. Utilisation simple allant de l'architecture légère à l'architecture extensible et distribuée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "# %matplotlib widget\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import optuna\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "from sklearn.utils import shuffle\n",
    "import sklearn.datasets\n",
    "import sklearn.ensemble\n",
    "import sklearn.model_selection\n",
    "import sklearn.svm\n",
    "from sklearn import datasets, svm, metrics\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import tree\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# visualisation\n",
    "from optuna.visualization.matplotlib import plot_contour\n",
    "from optuna.visualization.matplotlib import plot_edf\n",
    "from optuna.visualization.matplotlib import plot_intermediate_values\n",
    "from optuna.visualization.matplotlib import plot_optimization_history\n",
    "from optuna.visualization.matplotlib import plot_parallel_coordinate\n",
    "from optuna.visualization.matplotlib import plot_param_importances\n",
    "from optuna.visualization.matplotlib import plot_slice \n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_digits(images, count=4):\n",
    "    ncols = 4\n",
    "    nrows = (count + ncols - 1)//ncols\n",
    "    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(10, 3*nrows))\n",
    "    axes = axes.flatten()\n",
    "    for ax, image in zip(axes, images):\n",
    "        ax.set_axis_off()\n",
    "        ax.imshow(image., cmap=plt.cm.gray_r, interpolation=\"nearest\")\n",
    "        # ax.set_title(\"Digit: %i\" % label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = datasets.load_digits() # data images, target, target_names\n",
    "\n",
    "split_index = 1000\n",
    "x = digits.data\n",
    "y = digits.target\n",
    "x_train, x_test, y_train, y_test = sklearn.model_selection.train_test_split(x, y, train_size=0.5, random_state=0)\n",
    "\n",
    "plot_digits(digits.images)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introductory example\n",
    "\n",
    "As an introductory example, we use a Decision Tree Classifier to predict the digit from the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits_dt = tree.DecisionTreeClassifier(criterion='entropy', max_depth=20, min_samples_leaf=10)\n",
    "digits_dt.fit(x_train, y_train)\n",
    "prediction = digits_dt.predict(x_test)\n",
    "print(\"Generalization error:\", np.sum(np.not_equal(prediction, y_test))/len(y_test) )\n",
    "print(\"Generalization score:\", digits_dt.score(x_test, y_test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose that we want find the best hyperparameters to maximize the score. The most naive way to do it is by grid-search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_score = 0\n",
    "best_params = None\n",
    "nb_trials = 0\n",
    "for depth in [10, 20, 30]:\n",
    "    for min_samples_leaf in [3, 5, 10, 20]:\n",
    "        digits_dt = tree.DecisionTreeClassifier(criterion='entropy', max_depth=depth, min_samples_leaf=min_samples_leaf)\n",
    "        digits_dt.fit(x_train, y_train)\n",
    "        prediction = digits_dt.predict(x_test)\n",
    "        score = digits_dt.score(x_test, y_test)\n",
    "        if(score > best_score):\n",
    "            best_score = score\n",
    "            best_params = (depth, min_samples_leaf)\n",
    "        nb_trials += 1\n",
    "\n",
    "print('Best score={:0.2f} obtained for parameters={} after {} trials.'.format(score, best_params, nb_trials))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you see, we have to statically define the parameters to try and somewhat trial-and-error our way to the best solution, even with grid search. Of course, the Design of Experiment (DoE) can be improved quite easily, using latin hypercube for example. \n",
    "\n",
    "However, Optuna, an optimization framework, offers the possibility to ease this process in a very different way :\n",
    "- We define an objective function that calls, for the parameters we want to optimize on, the `suggest_API` and outputs a objevtive value (to minimize in this case).\n",
    "- Then, we define a study. \n",
    "- Then we can simply call the `optimize` function. By default it seeks to minimize the score.\n",
    "\n",
    "After the trial is over, finding the best set of parameters is done by calling member fields of the study object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_(trial, x, y):\n",
    "    sug_max_depth = trial.suggest_int('rf_max_depth', 2, 32)\n",
    "    sug_min_samples_leaf = trial.suggest_int('min_samples_leaf', 2, 20)\n",
    "\n",
    "    classifier_obj = sklearn.ensemble.RandomForestClassifier(max_depth=sug_max_depth, min_samples_leaf=sug_min_samples_leaf)\n",
    "    x_train, x_valid, y_train, y_valid = sklearn.model_selection.train_test_split(x, y, train_size=0.5, random_state=0)\n",
    "    classifier_obj.fit(x_train, y_train)\n",
    "    score = classifier_obj.score(x_valid, y_valid)\n",
    "    return score\n",
    "\n",
    "n_trials = 10\n",
    "objective = partial(objective_, x=x_train, y=y_train)\n",
    "study = optuna.create_study()  # Create a new study.\n",
    "study.optimize(lambda trial: 1.0 - objective(trial), n_trials=n_trials)  # Invoke optimization of the objective function.\n",
    "print('Best score={:0.2f} obtained for parameters={} after {} trials.'.format(1-study.best_value, study.best_params, n_trials))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So with the same number of trials, we found a better solution than what we could do with simple grid-search. Optuna also makes it easier to understand what goes on during the study with plots as seen afterwards. For more information, you can check [here](https://optuna.readthedocs.io/en/latest/tutorial/10_key_features/005_visualization.html#visualization). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_optimization_history(study);\n",
    "# plot_parallel_coordinate(study);\n",
    "# plot_param_importances(study);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optuna Search algorithm\n",
    "It is now time to discuss a bit more what goes on in Optunaw with the Suggest_API.\n",
    "\n",
    "[...]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What can we really do with Optuna ?\n",
    "\n",
    "Optuna can suggest three types of features ([source](https://optuna.readthedocs.io/en/latest/tutorial/10_key_features/002_configurations.html#sphx-glr-tutorial-10-key-features-002-configurations-py)):\n",
    "- categorical : `trial.suggest_categorical(str:name, choices:Sequence[Union[None, bool, int, float, str]])`\n",
    "- integers : `trial.suggest_int(name:str, low:int, high:int, step=1, log=False)`\n",
    "- floats : `trial.suggest_float(name:str, low:float, high:float, step=None, log=False)`\n",
    "\n",
    "\n",
    "The parameters they take in inputs are mostly quite clear :\n",
    "- *log* : if `True`, the value is sampled uniformely from the range in the log domain.\n",
    "- *choices : can be a sequence of anything, e.g. ['Random Forest Classifier', 'KNeightbors'] or [None, np.pi, 'SVM', 0, True].\n",
    "- high is **included**\n",
    "\n",
    "\n",
    "Next is an exercice. Propose an objective function `objective_ex1` that optimizes, for the NIST dataset, on :\n",
    "- Random Forest Classifier on the maximum depth and minimum samples in leaf\n",
    "- Gaussian Process Classifier on kernel (try for example the Radial Basis `RBF` with different value for the length scale)\n",
    "- Support Vector Classifier on the Kernel coefficient and Regularization parameter\n",
    "\n",
    "Run the study for 10 seconds (using the `timeout` parameter in the `study.optimize` function)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO : \n",
    "Ajouter la définition de ces fonctions dans des exercices avant celui-ci."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_sklearn(trial, obj, x, y):\n",
    "    x_train, x_valid, y_train, y_valid = sklearn.model_selection.train_test_split(x, y, train_size=0.5, random_state=0)\n",
    "    obj.fit(x_train, y_train)\n",
    "    score = obj.score(x_valid, y_valid)\n",
    "    return score\n",
    "\n",
    "def svc_objective(trial, x, y):\n",
    "    sug_C = trial.suggest_float('C', 1e-10, 1e10, log=True)\n",
    "    sug_gamma_kind = trial.suggest_categorical('gamma_kind', ['auto', 'scale', 'float'])\n",
    "    if(sug_gamma_kind == 'float'):\n",
    "        sug_gamma = trial.suggest_float('gamma', 1e-3, 10., log=True)\n",
    "    else:\n",
    "        sug_gamma = sug_gamma_kind\n",
    "    classifier_obj = sklearn.svm.SVC(C=sug_C, gamma=sug_gamma)\n",
    "    return train_sklearn(trial, classifier_obj, x, y)\n",
    "\n",
    "def random_forest_classifier_objective(trial, x, y):\n",
    "    sug_max_depth = trial.suggest_int('max_depth', 2, 32)\n",
    "    sug_min_samples_leaf = trial.suggest_int('min_samples_leaf', 2, 20)\n",
    "    classifier_obj = sklearn.ensemble.RandomForestClassifier(max_depth=sug_max_depth, min_samples_leaf=sug_min_samples_leaf)\n",
    "    return train_sklearn(trial, classifier_obj, x, y)\n",
    "\n",
    "def gaussian_Process_classifier_objective(trial, x, y):\n",
    "    sug_length_scale = trial.suggest_float('length_scale', 1e-3, 10, log=True)\n",
    "    kernel = RBF(length_scale=sug_length_scale)\n",
    "    classifier_obj = GaussianProcessClassifier(kernel=kernel)\n",
    "    return train_sklearn(trial, classifier_obj, x, y)\n",
    "\n",
    "def objective_ex1(trial, x, y):\n",
    "    classifier_name = trial.suggest_categorical('classifier', ['SVC', 'RandomForestClassifier', 'GaussianProcessClassifier'])\n",
    "    if(classifier_name == 'SVC'):\n",
    "        score = svc_objective(trial, x, y)\n",
    "    elif(classifier_name  == 'RandomForestClassifier'):\n",
    "        score = random_forest_classifier_objective(trial, x, y)\n",
    "    elif(classifier_name  == 'GaussianProcessClassifier'):\n",
    "        score = gaussian_Process_classifier_objective(trial, x, y)\n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeout = 10\n",
    "objective = partial(objective_ex1, x=x_train, y=y_train)\n",
    "study = optuna.create_study()  # Create a new study.\n",
    "study.optimize(lambda trial: 1.0 - objective(trial), timeout=timeout)  # Invoke optimization of the objective function.\n",
    "print('Best score={:0.5f} obtained for parameters={} after {} trials.'.format(1-study.best_value, study.best_params, n_trials))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# study.trials_dataframe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we see that we, very quickly, can get a very good classifier and quite easily in fact. But, that is not all that Optuna can do. There is also an overhead when creating the study object, as can be seen from the total duration compared to the choose timeout ! \n",
    "\n",
    "Indeed, Optuna has a pruning algorithm that allows it to cut short specific trials if it seens that intermediate results are not going so well. When done smartly, this allows to cut entire region of the parameter search space and thus explore much more quickly possibilities. It is to be noted that it can only be used when one can compute intermediate results.\n",
    "\n",
    "When dealing with the images dataset, it can make sense to use Deep Learning and more specifically, Convolutional Neural Network.\n",
    "\n",
    "More information [here](https://optuna.readthedocs.io/en/latest/tutorial/10_key_features/003_efficient_optimization_algorithms.html)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uping our game : MNIST\n",
    "Let's try on MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets as tdatasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_data = tdatasets.MNIST(\n",
    "    root = 'data',\n",
    "    train = True,                         \n",
    "    transform = ToTensor(), \n",
    "    download = True,            \n",
    ")\n",
    "test_data = tdatasets.MNIST(\n",
    "    root = 'data', \n",
    "    train = False,\n",
    "    transform = ToTensor()\n",
    ")\n",
    "\n",
    "# Limiting the size of the training set because it takes too much time otherwise !\n",
    "limit_train = 1000\n",
    "\n",
    "# subset training set\n",
    "index_sub = np.random.choice(np.arange(len(train_data)), limit_train, replace=False)\n",
    "\n",
    "#replacing attribute\n",
    "train_data.data = train_data.data[index_sub]\n",
    "train_data.targets = train_data.targets[index_sub]\n",
    "\n",
    "# train_data, valid_data = torch.utils.data.random_split(train_data_full, [limit_train, len(train_data_full) - limit_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_digits(train_data.data[:4].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_, y_train_ = train_data.data[:limit_train].numpy().reshape(limit_train, -1), train_data.targets[:limit_train].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeout = 10\n",
    "objective = partial(objective_ex1, x=x_train_, y=y_train_)\n",
    "study = optuna.create_study()  # Create a new study.\n",
    "study.optimize(lambda trial: 1.0 - objective(trial), timeout=timeout)  # Invoke optimization of the objective function.\n",
    "print('Best score={:0.5f} obtained for parameters={} after {} trials.'.format(1-study.best_value, study.best_params, n_trials))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we see that we don't succeed anymore in reaching top score. Let's now try with deep learning and convolutional neural networks..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_h_out(h_in, kernel_size, padding=0, dilation=1, stride=1):\n",
    "    return np.floor((h_in + 2 * padding - dilation * (kernel_size - 1) -1)/stride + 1)\n",
    "\n",
    "def get_w_out(w_in, kernel_size, padding=0, dilation=1, stride=1):\n",
    "    return np.floor((w_in + 2 * padding - dilation * (kernel_size - 1) -1)/stride + 1)\n",
    "\n",
    "class SmallConvNet(torch.nn.Module):\n",
    "    def __init__(self, c_in_list, c_out_list, kernel_size_list, output_size):\n",
    "        super(SmallConvNet, self).__init__()\n",
    "        conv_list = []\n",
    "        assert((len(c_in_list) == len(c_out_list)) and (len(kernel_size_list) == len(c_out_list)))\n",
    "        for c_in, c_out, k in zip(c_in_list, c_out_list, kernel_size_list):\n",
    "            conv_list.append(torch.nn.Conv2d(c_in, c_out, k))\n",
    "        self.conv = torch.nn.Sequential(*conv_list)\n",
    "        self.fc1 = torch.nn.Linear(output_size, 128)\n",
    "        self.fc2 = torch.nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        output = torch.nn.functional.softmax(x, dim=1)\n",
    "        return output\n",
    "\n",
    "class TorchClassifier:\n",
    "    def __init__(self, model, lr, verbose=True) -> None:\n",
    "        self.model = model\n",
    "        self.lr = lr\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model = self.model.to(self.device)\n",
    "        self.loss_function = torch.nn.CrossEntropyLoss()\n",
    "        self.optimiser = torch.optim.SGD(self.model.parameters(), lr=lr, momentum=0.9)\n",
    "        self.loss_function = torch.nn.CrossEntropyLoss()\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def train(self, train_loader, epochs):\n",
    "        train_loss_list = []\n",
    "        for epoch in tqdm(range(epochs), disable=not self.verbose):\n",
    "            train_loss = self.partial_fit(epoch, train_loader)\n",
    "            train_loss_list.append(train_loss)\n",
    "        return train_loss_list\n",
    "\n",
    "    def train_one_epoch(self, train_loader):\n",
    "        loss_list = []\n",
    "        for x, y in train_loader:\n",
    "            pred = self.model(x)\n",
    "            self.optimiser.zero_grad()\n",
    "            loss = self.loss_function(pred, y)\n",
    "            loss.backward()\n",
    "            self.optimiser.step()\n",
    "            loss_list.append(loss.detach().cpu().numpy())\n",
    "        return np.mean(loss_list)\n",
    "\n",
    "    def predict(self, test_loader, return_true=False):\n",
    "        self.model.eval()\n",
    "        pred_list = []\n",
    "        true_list = []\n",
    "        with torch.no_grad():\n",
    "            for x, y in test_loader:\n",
    "                pred = self.model(x)\n",
    "                pred_list.append(pred)\n",
    "                true_list.append(y)\n",
    "        self.model.train()\n",
    "        if(return_true):\n",
    "            return torch.concatenate(true_list, axis=0), torch.concatenate(pred_list, axis=0)\n",
    "        return torch.concatenate(pred_list, axis=0)\n",
    "    \n",
    "    def score(self, test_loader):\n",
    "        true, pred = self.predict(test_loader, return_true=True)\n",
    "        accuracy = get_accuracy(true.cpu().numpy(), pred.cpu().numpy())\n",
    "        return accuracy\n",
    "\n",
    "def get_accuracy(y_true, y_prob):\n",
    "    assert (y_true.ndim == 1 and y_true.shape[0] == y_prob.shape[0])\n",
    "    y_prob = np.argmax(y_prob, axis=-1)\n",
    "    return 1 - np.mean(np.abs(y_true - y_prob))\n",
    "\n",
    "\n",
    "def get_valid_predictions(net, validset):\n",
    "    validloader = torch.utils.data.DataLoader(validset, batch_size=4, shuffle=False)\n",
    "    all_labels = np.array([])\n",
    "    predictions = np.array([])\n",
    "    with torch.no_grad():\n",
    "        for data in validloader:\n",
    "            images, labels = data\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            all_labels = np.append(all_labels, labels.numpy())\n",
    "            predictions = np.append(predictions, predicted.numpy())\n",
    "    return all_labels, predictions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that, write an objective function, called `cnn_naive_objective`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_torch(trial, obj, train_data, valid_data):\n",
    "    train_loader = torch.utils.data.DataLoader(train_data, \n",
    "                                              batch_size=16, \n",
    "                                              shuffle=True, \n",
    "                                              num_workers=1)\n",
    "    valid_loader = torch.utils.data.DataLoader(valid_data, \n",
    "                                               batch_size=16, \n",
    "                                               shuffle=True, \n",
    "                                               num_workers=1)\n",
    "    for step in range(3):\n",
    "        obj.train_one_epoch(train_loader)\n",
    "        intermediate_value = obj.score(valid_loader)\n",
    "            \n",
    "        trial.report(intermediate_value, step)\n",
    "\n",
    "        # Handle pruning based on the intermediate value.\n",
    "        if trial.should_prune():\n",
    "            raise optuna.TrialPruned()\n",
    "\n",
    "    return 1.0 - obj.score(valid_loader)\n",
    "\n",
    "def cnn_naive_objective(trial, train_data, valid_data):\n",
    "    n_layers = trial.suggest_int('n_layers', 1, 3)\n",
    "    c_in_list = [1]\n",
    "    c_out_list = []\n",
    "    k_list = []\n",
    "    h = train_data.data[0].shape[0]\n",
    "    w = train_data.data[0].shape[1]\n",
    "    for n in range(n_layers):\n",
    "        if(n != 0):\n",
    "            c_in_list.append(c_out_list[-1])\n",
    "        c_out_list.append(trial.suggest_int(f'c_out_{n}', 1, 4))\n",
    "        k_list.append(trial.suggest_int(f'k_{n}', 3, 7, 2))\n",
    "        w = get_w_out(w, k_list[-1])\n",
    "        h = get_h_out(h, k_list[-1])\n",
    "\n",
    "    convnet = SmallConvNet(\n",
    "        c_in_list=c_in_list,\n",
    "        c_out_list=c_out_list,\n",
    "        kernel_size_list=k_list,\n",
    "        output_size=int(h*w*c_out_list[-1])\n",
    "    )\n",
    "\n",
    "    sug_lr = trial.suggest_float('lr', 1e-5, 1, log=True)\n",
    "    classifier = TorchClassifier(\n",
    "        convnet,\n",
    "        lr=sug_lr\n",
    "    )\n",
    "    \n",
    "    return train_torch(trial, classifier, train_data, valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeout = 120\n",
    "objective = partial(cnn_naive_objective, train_data=train_data, valid_data=test_data)\n",
    "study = optuna.create_study(pruner=optuna.pruners.MedianPruner())\n",
    "study.optimize(lambda trial: 1.0 - objective(trial), timeout=timeout, show_progress_bar=True)  # Invoke optimization of the objective function.\n",
    "print('Best score={:0.5f} obtained for parameters={} after {} trials.'.format(1-study.best_value, study.best_params, n_trials))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "optunenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16 (main, Jan 11 2023, 16:05:54) \n[GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "44b004f380dd4e5778bdd86936824a5c6b89fcc73ab0c0c017a27da2768be855"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
