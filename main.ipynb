{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OPTUNA \n",
    "\n",
    "MIT licence.\n",
    "\n",
    "3 principaux objectifs :\n",
    "1. Construction de l'espace de recherche de façon dynamique (*define-by-run API*)\n",
    "2. Implémentation efficiente de la recherche mais également du *pruning* (c'est-à-dire quand des branches entières de l'espace de recherche sont supprimées)\n",
    "3. Utilisation simple allant de l'architecture légère à l'architecture extensible et distribuée.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To install : \n",
    "\n",
    "```shell\n",
    "conda create -n optunenv python=3.9\n",
    "conda install -c conda-forge optuna\n",
    "conda install pandas\n",
    "conda install pytorch torchvision torchaudio cpuonly -c pytorch\n",
    "conda install -c anaconda scikit-learn\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paul/miniconda3/envs/optunenv/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "import sklearn.datasets\n",
    "import sklearn.ensemble\n",
    "import sklearn.model_selection\n",
    "import sklearn.svm\n",
    "\n",
    "import torch\n",
    "import mytorch\n",
    "import optuna\n",
    "\n",
    "path_to_train_data = Path(\"data/Earthquakes_TRAIN.txt\")\n",
    "path_to_test_data = Path(\"data/Earthquakes_TEST.txt\")\n",
    "\n",
    "arr_train = np.loadtxt(path_to_train_data)\n",
    "arr_test = np.loadtxt(path_to_test_data)\n",
    "y_train = arr_train[:, 0].astype(int)\n",
    "x_train = arr_train[:, 1:]\n",
    "y_test = arr_test[:, 0].astype(int)\n",
    "x_test = arr_test[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_layers = [1, 2, 3, 4]\n",
    "hidden_sizes = [1, 2, 3, 4]\n",
    "lr_list = [1e-1, 1e-2, 1e-3, 1e-4]\n",
    "batch_sizes = [4, 8, 16]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by defining the data, models, metrics used to illustrate OPTUNA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_class = partial(mytorch.TimeSeriesClassificationNet, input_size=1, number_of_classes=2, activation_fn='logsoftmax')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = model_class(hidden_size=hidden_sizes[0], num_layers=number_of_layers[0])\n",
    "# mytorch.train(model, x_train, y_train, lr_list[0], epochs=10, batch_size=batch_sizes[0], lengths=[0.6, 0.4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-01-28 19:24:07,662]\u001b[0m A new study created in memory with name: no-name-1b1e865f-3767-4f1b-a8a7-e9706fac39ae\u001b[0m\n",
      "\u001b[33m[W 2023-01-28 19:25:03,784]\u001b[0m Trial 0 failed with parameters: {'classifier': 'GRU-classification-head', 'hidden_size': 5, 'num_layers': 7, 'lr': 0.00032370749956264647} because of the following error: KeyboardInterrupt().\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/paul/miniconda3/envs/optunenv/lib/python3.9/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_136648/2050738999.py\", line 19, in objective_\n",
      "    classifier_obj.fit(x_train, y_train)\n",
      "  File \"/home/paul/Documents/repertoires/supaero-sdd-notebook-ml/mytorch.py\", line 14, in fit\n",
      "    train(self.model, x, y, self.lr, self.epochs, self.batch_size, self.device)\n",
      "  File \"/home/paul/Documents/repertoires/supaero-sdd-notebook-ml/mytorch.py\", line 39, in train\n",
      "    train_loss = train_one_epoch(epoch, model, optimiser, loss_function, train_loader)\n",
      "  File \"/home/paul/Documents/repertoires/supaero-sdd-notebook-ml/mytorch.py\", line 60, in train_one_epoch\n",
      "    pred = model(x)\n",
      "  File \"/home/paul/miniconda3/envs/optunenv/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/paul/Documents/repertoires/supaero-sdd-notebook-ml/mytorch.py\", line 95, in forward\n",
      "    X, h = self.rnn(X) # size is batch size x sequence length x output_size = input_size\n",
      "  File \"/home/paul/miniconda3/envs/optunenv/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/paul/miniconda3/envs/optunenv/lib/python3.9/site-packages/torch/nn/modules/rnn.py\", line 955, in forward\n",
      "    result = _VF.gru(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "KeyboardInterrupt\n",
      "\u001b[33m[W 2023-01-28 19:25:03,808]\u001b[0m Trial 0 failed with value None.\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 27\u001b[0m\n\u001b[1;32m     25\u001b[0m objective \u001b[39m=\u001b[39m partial(objective_, x\u001b[39m=\u001b[39mx_train, y\u001b[39m=\u001b[39my_train)\n\u001b[1;32m     26\u001b[0m study \u001b[39m=\u001b[39m optuna\u001b[39m.\u001b[39mcreate_study()  \u001b[39m# Create a new study.\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m study\u001b[39m.\u001b[39;49moptimize(objective, n_trials\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/optunenv/lib/python3.9/site-packages/optuna/study/study.py:425\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moptimize\u001b[39m(\n\u001b[1;32m    322\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    323\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    330\u001b[0m     show_progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    331\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    332\u001b[0m     \u001b[39m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    333\u001b[0m \n\u001b[1;32m    334\u001b[0m \u001b[39m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[39m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    423\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 425\u001b[0m     _optimize(\n\u001b[1;32m    426\u001b[0m         study\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m    427\u001b[0m         func\u001b[39m=\u001b[39;49mfunc,\n\u001b[1;32m    428\u001b[0m         n_trials\u001b[39m=\u001b[39;49mn_trials,\n\u001b[1;32m    429\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    430\u001b[0m         n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[1;32m    431\u001b[0m         catch\u001b[39m=\u001b[39;49m\u001b[39mtuple\u001b[39;49m(catch) \u001b[39mif\u001b[39;49;00m \u001b[39misinstance\u001b[39;49m(catch, Iterable) \u001b[39melse\u001b[39;49;00m (catch,),\n\u001b[1;32m    432\u001b[0m         callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m    433\u001b[0m         gc_after_trial\u001b[39m=\u001b[39;49mgc_after_trial,\n\u001b[1;32m    434\u001b[0m         show_progress_bar\u001b[39m=\u001b[39;49mshow_progress_bar,\n\u001b[1;32m    435\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/optunenv/lib/python3.9/site-packages/optuna/study/_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     65\u001b[0m     \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m---> 66\u001b[0m         _optimize_sequential(\n\u001b[1;32m     67\u001b[0m             study,\n\u001b[1;32m     68\u001b[0m             func,\n\u001b[1;32m     69\u001b[0m             n_trials,\n\u001b[1;32m     70\u001b[0m             timeout,\n\u001b[1;32m     71\u001b[0m             catch,\n\u001b[1;32m     72\u001b[0m             callbacks,\n\u001b[1;32m     73\u001b[0m             gc_after_trial,\n\u001b[1;32m     74\u001b[0m             reseed_sampler_rng\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     75\u001b[0m             time_start\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m     76\u001b[0m             progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[1;32m     77\u001b[0m         )\n\u001b[1;32m     78\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     79\u001b[0m         \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/optunenv/lib/python3.9/site-packages/optuna/study/_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 163\u001b[0m     frozen_trial \u001b[39m=\u001b[39m _run_trial(study, func, catch)\n\u001b[1;32m    164\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    165\u001b[0m     \u001b[39m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[39m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    167\u001b[0m     \u001b[39m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    168\u001b[0m     \u001b[39m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     \u001b[39mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m~/miniconda3/envs/optunenv/lib/python3.9/site-packages/optuna/study/_optimize.py:251\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mFalse\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mShould not reach.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    246\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    247\u001b[0m     frozen_trial\u001b[39m.\u001b[39mstate \u001b[39m==\u001b[39m TrialState\u001b[39m.\u001b[39mFAIL\n\u001b[1;32m    248\u001b[0m     \u001b[39mand\u001b[39;00m func_err \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    249\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    250\u001b[0m ):\n\u001b[0;32m--> 251\u001b[0m     \u001b[39mraise\u001b[39;00m func_err\n\u001b[1;32m    252\u001b[0m \u001b[39mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m~/miniconda3/envs/optunenv/lib/python3.9/site-packages/optuna/study/_optimize.py:200\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[39mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[39m.\u001b[39m_trial_id, study\u001b[39m.\u001b[39m_storage):\n\u001b[1;32m    199\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 200\u001b[0m         value_or_values \u001b[39m=\u001b[39m func(trial)\n\u001b[1;32m    201\u001b[0m     \u001b[39mexcept\u001b[39;00m exceptions\u001b[39m.\u001b[39mTrialPruned \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    202\u001b[0m         \u001b[39m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    203\u001b[0m         state \u001b[39m=\u001b[39m TrialState\u001b[39m.\u001b[39mPRUNED\n",
      "Cell \u001b[0;32mIn[5], line 19\u001b[0m, in \u001b[0;36mobjective_\u001b[0;34m(trial, x, y)\u001b[0m\n\u001b[1;32m     16\u001b[0m     classifier_obj \u001b[39m=\u001b[39m mytorch\u001b[39m.\u001b[39mTorchClassifier(model, lr, epochs, batch_size)\n\u001b[1;32m     18\u001b[0m x_train, x_valid, y_train, y_valid \u001b[39m=\u001b[39m sklearn\u001b[39m.\u001b[39mmodel_selection\u001b[39m.\u001b[39mtrain_test_split(x, y, random_state\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m---> 19\u001b[0m classifier_obj\u001b[39m.\u001b[39;49mfit(x_train, y_train)\n\u001b[1;32m     20\u001b[0m y_pred \u001b[39m=\u001b[39m classifier_obj\u001b[39m.\u001b[39mpredict(x_valid)\n\u001b[1;32m     22\u001b[0m error \u001b[39m=\u001b[39m sklearn\u001b[39m.\u001b[39mmetrics\u001b[39m.\u001b[39mlog_loss(y_valid, y_pred)\n",
      "File \u001b[0;32m~/Documents/repertoires/supaero-sdd-notebook-ml/mytorch.py:14\u001b[0m, in \u001b[0;36mTorchClassifier.fit\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, x, y):\n\u001b[0;32m---> 14\u001b[0m     train(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel, x, y, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlr, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mepochs, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_size, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice)\n",
      "File \u001b[0;32m~/Documents/repertoires/supaero-sdd-notebook-ml/mytorch.py:39\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, x, y, lr, epochs, batch_size, device)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[39m# valid_loss_list = []\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[0;32m---> 39\u001b[0m     train_loss \u001b[39m=\u001b[39m train_one_epoch(epoch, model, optimiser, loss_function, train_loader)\n\u001b[1;32m     40\u001b[0m     \u001b[39m# valid_loss = evaluate(valid_loader, model, loss_function)\u001b[39;00m\n\u001b[1;32m     42\u001b[0m     train_loss_list\u001b[39m.\u001b[39mappend(train_loss)\n",
      "File \u001b[0;32m~/Documents/repertoires/supaero-sdd-notebook-ml/mytorch.py:60\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(epoch_index, model, optimiser, loss_function, data_loader)\u001b[0m\n\u001b[1;32m     58\u001b[0m loss_list \u001b[39m=\u001b[39m []\n\u001b[1;32m     59\u001b[0m \u001b[39mfor\u001b[39;00m x, y \u001b[39min\u001b[39;00m data_loader:\n\u001b[0;32m---> 60\u001b[0m     pred \u001b[39m=\u001b[39m model(x)\n\u001b[1;32m     61\u001b[0m     optimiser\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     62\u001b[0m     loss \u001b[39m=\u001b[39m loss_function(pred, y)\n",
      "File \u001b[0;32m~/miniconda3/envs/optunenv/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/repertoires/supaero-sdd-notebook-ml/mytorch.py:95\u001b[0m, in \u001b[0;36mTimeSeriesClassificationNet.forward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[39mif\u001b[39;00m(\u001b[39mlen\u001b[39m(X\u001b[39m.\u001b[39mshape) \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m):\n\u001b[1;32m     94\u001b[0m     X \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39munsqueeze(X, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m---> 95\u001b[0m X, h \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrnn(X) \u001b[39m# size is batch size x sequence length x output_size = input_size\u001b[39;00m\n\u001b[1;32m     96\u001b[0m X \u001b[39m=\u001b[39m X[:, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, :] \u001b[39m# selecting last output\u001b[39;00m\n\u001b[1;32m     97\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_layer(X)\n",
      "File \u001b[0;32m~/miniconda3/envs/optunenv/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/optunenv/lib/python3.9/site-packages/torch/nn/modules/rnn.py:955\u001b[0m, in \u001b[0;36mGRU.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    953\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_forward_args(\u001b[39minput\u001b[39m, hx, batch_sizes)\n\u001b[1;32m    954\u001b[0m \u001b[39mif\u001b[39;00m batch_sizes \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 955\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39;49mgru(\u001b[39minput\u001b[39;49m, hx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flat_weights, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_layers,\n\u001b[1;32m    956\u001b[0m                      \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbidirectional, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_first)\n\u001b[1;32m    957\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    958\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39mgru(\u001b[39minput\u001b[39m, batch_sizes, hx, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_weights, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias,\n\u001b[1;32m    959\u001b[0m                      \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbidirectional)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def objective_(trial, x, y):\n",
    "    classifier_name = trial.suggest_categorical('classifier', ['SVR', 'RandomForest', 'GRU-classification-head'])\n",
    "    if(classifier_name == 'SVR'):\n",
    "        svr_c = trial.suggest_float('svr_c', 1e-10, 1e10, log=True)\n",
    "        classifier_obj = sklearn.svm.SVR(C=svr_c)\n",
    "    elif(classifier_name  == 'RandomForest'):\n",
    "        rf_max_depth = trial.suggest_int('rf_max_depth', 2, 32)\n",
    "        classifier_obj = sklearn.ensemble.RandomForestClassifier(max_depth=rf_max_depth)\n",
    "    else:\n",
    "        hidden_size = trial.suggest_int('hidden_size', 1, 8)\n",
    "        num_layers = trial.suggest_int('num_layers', 1, 8)\n",
    "        lr = trial.suggest_float('lr', 1e-5, 1, log=True)\n",
    "        epochs = 10\n",
    "        batch_size = 8\n",
    "        model = model_class(hidden_size=hidden_size, num_layers=num_layers)\n",
    "        classifier_obj = mytorch.TorchClassifier(model, lr, epochs, batch_size)\n",
    "\n",
    "    x_train, x_valid, y_train, y_valid = sklearn.model_selection.train_test_split(x, y, random_state=0)\n",
    "    classifier_obj.fit(x_train, y_train)\n",
    "    y_pred = classifier_obj.predict(x_valid)\n",
    "\n",
    "    error = sklearn.metrics.log_loss(y_valid, y_pred)\n",
    "    return error\n",
    "\n",
    "objective = partial(objective_, x=x_train, y=y_train)\n",
    "study = optuna.create_study()  # Create a new study.\n",
    "study.optimize(objective, n_trials=100)  # Invoke optimization of the objective function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "optunenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16 (main, Jan 11 2023, 16:05:54) \n[GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "44b004f380dd4e5778bdd86936824a5c6b89fcc73ab0c0c017a27da2768be855"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
